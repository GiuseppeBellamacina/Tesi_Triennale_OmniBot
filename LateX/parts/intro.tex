\chapter*{Introduzione}
\markboth{\MakeUppercase{Introduzione}}{\MakeUppercase{Introduzione}}
\addcontentsline{toc}{chapter}{Introduzione}
Lo sviluppo di tecnologie avanzate nel campo dell'apprendimento automatico ha portato a cambiamenti radicali nel modo in cui i sistemi intelligenti interagiscono con il mondo circostante. In questa tesi verrà trattata inizialmente la teoria alla base dei Large Language Models (LLM), analizzando le principali tappe che hanno portato alla loro evoluzione. Si esamineranno alcuni dei modelli predecessori, evidenziando le sfide affrontate e le soluzioni trovate per superare i loro limiti. Questi modelli, pur rappresentando importanti passi avanti, presentavano difficoltà significative nel gestire contesti complessi e nel catturare dipendenze linguistiche a lungo raggio, problemi che sono stati in gran parte risolti con l'introduzione del modello Transformer.

Quest'ultimo, grazie al suo innovativo meccanismo di attenzione, ha portato ad una rivoluzione nel campo del Natural Language Processing (NLP) poiché consente di catturare con grande precisione le dipendenze tra le parole, anche quando queste sono distanti all'interno di una frase. Questo ha consentito di superare i limiti strutturali dei modelli precedenti, come le reti ricorrenti, e ha gettato le basi per la creazione di modelli linguistici di dimensioni e capacità senza precedenti, i cosiddetti Large Language Models.

Tuttavia, nonostante la potenza degli LLM, essi presentano ancora una limitazione fondamentale: il loro apprendimento si basa su grandi quantità di dati statici, e non sono in grado di aggiornare o acquisire nuove informazioni in modo dinamico senza essere riaddestrati. Per rispondere a questa sfida, è stata introdotta una tecnica innovativa, la Retrieval-Augmented Generation (RAG), che ottimizza il processo di accesso alle informazioni. A differenza dei metodi tradizionali, che richiedono ai modelli di memorizzare tutto ciò che hanno appreso, la RAG sfrutta un database esterno per recuperare in tempo reale le informazioni necessarie. Si elimina il bisogno di fornire un apprendimento continuo al modello stesso. Questa tecnica non solo rende i modelli più efficienti e scalabili, ma permette loro di essere aggiornati e personalizzati in modo dinamico e veloce.

Dopo aver discusso le teorie alla base di questi concetti, la tesi si concentrerà su un'applicazione pratica concreta: lo sviluppo di un chatbot chiamato "OmniBot", nato a partire da un progetto assegnato durante un tirocinio curriculare presso l'azienda Intellisync \cite{intellisync}, basato sull'integrazione dell'architettura RAG all'interno di una catena di pensiero personalizzata, nota come "Chain-of-Thoughts" (CoT). Verranno analizzate dettagliatamente tutte le fasi del processo di sviluppo, dai primi esperimenti fino alla creazione del prototipo finale. Ogni passaggio del processo evolutivo sarà esplorato, mettendo in relazione gli accorgimenti progettuali adottati con le difficoltà che li hanno resi necessari e come le soluzioni scelte per affrontare i problemi riscontrati abbiano contribuito a migliorare il sistema complessivo.

L'implementazione di OmniBot rappresenta non solo una dimostrazione pratica dell'efficacia della RAG e della CoT, ma anche un'approfondita indagine su come queste tecnologie possano essere combinate per migliorare significativamente la coerenza, la pertinenza e l'accuratezza delle risposte generate dal chatbot. In particolare, verrà illustrato come l'integrazione di un database esterno con una struttura di ragionamento basata sulla catena di pensiero consenta di gestire in modo dinamico il recupero delle informazioni, mantenendo al contempo un controllo mirato sul comportamento del modello. Questo approccio garantisce una maggiore flessibilità e adattabilità rispetto ai modelli tradizionali e permette una gestione più precisa delle risposte in base al contesto e agli obiettivi prefissati.

Oltre alla descrizione tecnica, la tesi si concentrerà anche sull'analisi critica dei risultati ottenuti. Verranno presentati i dati emersi dai test sperimentali condotti su OmniBot, con l'obiettivo di dimostrare come l'approccio combinato di RAG e CoT possa essere applicato efficacemente in contesti reali, migliorando l'esperienza dell'utente e ottimizzando le performance del sistema. Le sperimentazioni non solo confermeranno la validità teorica del modello, ma forniranno anche indicazioni preziose su possibili miglioramenti futuri, mettendo in luce le aree in cui sono necessari ulteriori sviluppi o adattamenti.

Infine, la tesi esplorerà le potenziali evoluzioni di queste tecnologie, proiettandosi verso il futuro delle architetture di LLM potenziate. Verranno analizzate le sfide ancora aperte, come l'incremento della capacità di gestione di contesti ancora più complessi o l'introduzione di meccanismi avanzati di supervisione, così come le opportunità che potrebbero emergere con l'integrazione di tecniche di ottimizzazione più sofisticate. Le conclusioni indicheranno non solo i miglioramenti concreti raggiunti nel corso del progetto, ma anche le direzioni in cui queste tecnologie potranno evolversi, contribuendo ulteriormente all'avanzamento del campo dell'NLP.