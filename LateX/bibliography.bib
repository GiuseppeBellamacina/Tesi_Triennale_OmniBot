@misc{vaswani2023attentionneed,
  title        = {Attention Is All You Need},
  author       = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year         = {2023},
  eprint       = {1706.03762},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {\url{https://arxiv.org/abs/1706.03762}},
}

@misc{bahdanau2016neuralmachinetranslationjointly,
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate}, 
  author       = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  year         = {2016},
  eprint       = {1409.0473},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {\url{https://arxiv.org/abs/1409.0473}},
}

@misc{luong2015effectiveapproachesattentionbasedneural,
  title        = {Effective Approaches to Attention-based Neural Machine Translation}, 
  author       = {Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
  year         = {2015},
  eprint       = {1508.04025},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {\url{https://arxiv.org/abs/1508.04025}},
}

@misc{mpnet,
  title       = {sentence-transformers/all-mpnet-base-v2},
  author      = {Sentence Transformers},
  year        = {2021},
  note       = {\url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}},
}

@misc{transformersstateofarthuggingface,
  title        = {Transformers: State-of-the-Art Natural Language Processing},
  author       = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Remi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander Rush},
  year         = {2020},
  note         = {\url{https://aclanthology.org/2020.emnlp-demos.6}},
}

@misc{neo4j,
  title        = {Neo4j},
  author       = {Neo4j},
  year         = {2024},
  note         = {\url{https://neo4j.com}},
}

@misc{hsieh2023distillingstepbystepoutperforminglarger,
  title={Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes}, 
  author={Cheng-Yu Hsieh and Chun-Liang Li and Chih-Kuan Yeh and Hootan Nakhost and Yasuhisa Fujii and Alexander Ratner and Ranjay Krishna and Chen-Yu Lee and Tomas Pfister},
  year={2023},
  eprint={2305.02301},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2305.02301},
  note = {\url{https://arxiv.org/abs/2305.02301}},
}

@misc{ruoss2024grandmasterlevelchesssearch,
  title        = {Grandmaster-Level Chess Without Search}, 
  author       = {Anian Ruoss and Grégoire Delétang and Sourabh Medapati and Jordi Grau-Moya and Li Kevin Wenliang and Elliot Catt and John Reid and Tim Genewein},
  year         = {2024},
  eprint       = {2402.04494},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  note         =  {\url{https://arxiv.org/abs/2402.04494}}, 
}

@misc{wu2016googlesneuralmachinetranslation,
  title        = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}, 
  author       = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
  year         = {2016},
  eprint       = {1609.08144},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         =  {\url{https://arxiv.org/abs/1609.08144}}, 
}

@misc{Khan_2022,
  title        = {Transformers in Vision: A Survey},
  author       = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  year         = {2022},
  eprint       = {2101.01169},
  archivePrefix= {arXiv},
  primaryClass = {cs.CV},
  note         = {\url{https://arxiv.org/abs/2101.01169}},
}

@misc{gulati2020conformerconvolutionaugmentedtransformerspeech,
  title        = {Conformer: Convolution-augmented Transformer for Speech Recognition}, 
  author       = {Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},
  year         = {2020},
  eprint       = {2005.08100},
  archivePrefix= {arXiv},
  primaryClass = {eess.AS},
  note         = {\url{https://arxiv.org/abs/2005.08100}},
}

@misc{chen2021decisiontransformerreinforcementlearning,
  title        = {Decision Transformer: Reinforcement Learning via Sequence Modeling}, 
  author       = {Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
  year         = {2021},
  eprint       = {2106.01345},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  note         = {\url{https://arxiv.org/abs/2106.01345}},
}

@misc{choromanski2022rethinkingattentionperformers,
  title        = {Rethinking Attention with Performers}, 
  author       = {Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
  year         = {2022},
  eprint       = {2009.14794},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  note         = {\url{https://arxiv.org/abs/2009.14794}},
}

@misc{esser2024scalingrectifiedflowtransformers,
  title        = {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis}, 
  author       = {Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas Müller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Kyle Lacey and Alex Goodwin and Yannik Marek and Robin Rombach},
  year         = {2024},
  eprint       = {2403.03206},
  archivePrefix= {arXiv},
  primaryClass = {cs.CV},
  note         = {\url{https://arxiv.org/abs/2403.03206}}, 
}

@misc{gpt2backbone,
  title        = {GPT2Backbone Model},
  author       = {OpenAI and Keras},
  year         = {2022},
  note         = {\url{https://github.com/keras-team/keras-hub/blob/v0.15.1/keras_nlp/src/models/gpt2/gpt2_backbone.py}},
}

@misc{devlin2019bertpretrainingdeepbidirectional,
  title        = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
  author       = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year         = {2019},
  eprint       = {1810.04805},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {\url{https://arxiv.org/abs/1810.04805}}, 
}

@misc{mikolov2013efficientestimationwordrepresentations,
  title={Efficient Estimation of Word Representations in Vector Space}, 
  author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  year={2013},
  eprint={1301.3781},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1301.3781},
  note = {\url{https://arxiv.org/abs/1301.3781}},
}

@inproceedings{pennington-etal-2014-glove,
  title = "{G}lo{V}e: Global Vectors for Word Representation",
  author = "Pennington, Jeffrey  and
    Socher, Richard  and
    Manning, Christopher",
  editor = "Moschitti, Alessandro  and
    Pang, Bo  and
    Daelemans, Walter",
  booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
  month = oct,
  year = "2014",
  address = "Doha, Qatar",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D14-1162",
  note = {\url{https://aclanthology.org/D14-1162}},
  doi = "10.3115/v1/D14-1162",
  pages = "1532--1543",
}

@misc{camachocollados2018wordsenseembeddingssurvey,
  title        = {From Word to Sense Embeddings: A Survey on Vector Representations of Meaning}, 
  author       = {Jose Camacho-Collados and Mohammad Taher Pilehvar},
  year         = {2018},
  eprint       = {1805.04032},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {\url{https://arxiv.org/abs/1805.04032}}, 
}

@inproceedings{reisinger-mooney-2010-multi,
  title = "Multi-Prototype Vector-Space Models of Word Meaning",
  author = "Reisinger, Joseph  and
    Mooney, Raymond J.",
  editor = "Kaplan, Ron  and
    Burstein, Jill  and
    Harper, Mary  and
    Penn, Gerald",
  booktitle = "Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
  month = jun,
  year = "2010",
  address = "Los Angeles, California",
  publisher = "Association for Computational Linguistics",
  pages = "109--117",
  note = {\url{https://aclanthology.org/N10-1013}}, 
}

@misc{noamlearningrate,
  title        = {Noam Learning Rate Scheduler},
  author       = {Noam Shazeer},
  year         = {2017},
  note         = {\url{https://nn.labml.ai/optimizers/noam.html}},
}

@misc{10.1145/3442188.3445922,
  author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  year = {2021},
  isbn = {9781450383097},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3442188.3445922},
  note = {\url{https://doi.org/10.1145/3442188.3445922}},
  doi = {10.1145/3442188.3445922},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages = {610-623},
  numpages = {14},
  location = {Virtual Event, Canada},
  series = {FAccT '21}
}

@misc{xiong2020layernormalizationtransformerarchitecture,
  title        = {On Layer Normalization in the Transformer Architecture}, 
  author       = {Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie-Yan Liu},
  year         = {2020},
  eprint       = {2002.04745},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  note         = {\url{https://arxiv.org/abs/2002.04745}},
}

@article{10.1162/neco.1997.9.8.1735,
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title = "{Long Short-Term Memory}",
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735-1780},
  year = {1997},
  month = {11},
  abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://doi.org/10.1162/neco.1997.9.8.1735},
  eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
  note = {\url{https://doi.org/10.1162/neco.1997.9.8.1735}},
}

@misc{sak2014longshorttermmemorybased,
  title={Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}, 
  author={Haşim Sak and Andrew Senior and Françoise Beaufays},
  year={2014},
  eprint={1402.1128},
  archivePrefix={arXiv},
  primaryClass={cs.NE},
  url={https://arxiv.org/abs/1402.1128},
  note = {\url{https://arxiv.org/abs/1402.1128}}, 
}

@misc{stolcke1994bestfirstmodelmerginghidden,
  title={Best-first Model Merging for Hidden Markov Model Induction}, 
  author={Andreas Stolcke and Stephen M. Omohundro},
  year={1994},
  eprint={cmp-lg/9405017},
  archivePrefix={arXiv},
  primaryClass={cmp-lg},
  url={https://arxiv.org/abs/cmp-lg/9405017},
  note = {\url{https://arxiv.org/abs/cmp-lg/9405017}},
}

@misc{minaee2024largelanguagemodelssurvey,
  title={Large Language Models: A Survey}, 
  author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
  year={2024},
  eprint={2402.06196},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2402.06196}, 
  note = {\url{https://arxiv.org/abs/2402.06196}},
}

@misc{cheng2016longshorttermmemorynetworksmachine,
  title={Long Short-Term Memory-Networks for Machine Reading}, 
  author={Jianpeng Cheng and Li Dong and Mirella Lapata},
  year={2016},
  eprint={1601.06733},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1601.06733},
  note = {\url{https://arxiv.org/abs/1601.06733}},
}

@misc{radford2022robustspeechrecognitionlargescale,
  title={Robust Speech Recognition via Large-Scale Weak Supervision}, 
  author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
  year={2022},
  eprint={2212.04356},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  url={https://arxiv.org/abs/2212.04356},
  note = {\url{https://arxiv.org/abs/2212.04356}}, 
}

@misc{jaegle2021perceivergeneralperceptioniterative,
  title={Perceiver: General Perception with Iterative Attention}, 
  author={Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira},
  year={2021},
  eprint={2103.03206},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2103.03206},
  note = {\url{https://arxiv.org/abs/2103.03206}}, 
}

@misc{jaegle2022perceiveriogeneralarchitecture,
  title={Perceiver IO: A General Architecture for Structured Inputs and Outputs}, 
  author={Andrew Jaegle and Sebastian Borgeaud and Jean-Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Daniel Zoran and Andrew Brock and Evan Shelhamer and Olivier Hénaff and Matthew M. Botvinick and Andrew Zisserman and Oriol Vinyals and Joāo Carreira},
  year={2022},
  eprint={2107.14795},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2107.14795},
  note = {\url{https://arxiv.org/abs/2107.14795}},
}

@misc{cho2014learningphraserepresentationsusing,
  title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}, 
  author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  year={2014},
  eprint={1406.1078},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1406.1078},
  note = {\url{https://arxiv.org/abs/1406.1078}}, 
}

@misc{sutskever2014sequencesequencelearningneural,
  title={Sequence to Sequence Learning with Neural Networks}, 
  author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  year={2014},
  eprint={1409.3215},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1409.3215},
  note = {\url{https://arxiv.org/abs/1409.3215}},
}

@misc{bertattention,
  title={What Does BERT Look At? An Analysis of BERT's Attention}, 
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  year={2019},
  eprint={1906.04341},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1906.04341},
  note = {\url{https://arxiv.org/abs/1906.04341}},
}

@misc{hendrycks2023gaussianerrorlinearunits,
  title={Gaussian Error Linear Units (GELUs)}, 
  author={Dan Hendrycks and Kevin Gimpel},
  year={2023},
  eprint={1606.08415},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1606.08415},
  note = {\url{https://arxiv.org/abs/1606.08415}},
}

@misc{zhang2019rootmeansquarelayer,
  title={Root Mean Square Layer Normalization}, 
  author={Biao Zhang and Rico Sennrich},
  year={2019},
  eprint={1910.07467},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1910.07467}, 
  note = {\url{https://arxiv.org/abs/1910.07467}},
}

@misc{https://doi.org/10.5281/zenodo.3525484,
  doi = {10.5281/ZENODO.3525484},
  url = {https://zenodo.org/record/3525484},
  author = {Nguyen, Toan Q. and Salazar, Julian},
  language = {en},
  title = {Transformers without Tears: Improving the Normalization of Self-Attention},
  publisher = {Zenodo},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International},
  note = {\url{https://zenodo.org/record/3525484}},
}

@misc{ba2016layernormalization,
  title={Layer Normalization}, 
  author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  year={2016},
  eprint={1607.06450},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/1607.06450},
  note = {\url{https://arxiv.org/abs/1607.06450}},
}

@misc{phuong2022formalalgorithmstransformers,
  title={Formal Algorithms for Transformers}, 
  author={Mary Phuong and Marcus Hutter},
  year={2022},
  eprint={2207.09238},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2207.09238},
  note = {\url{https://arxiv.org/abs/2207.09238}},
}

@misc{raffel2023exploringlimitstransferlearning,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
  author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  year={2023},
  eprint={1910.10683},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1910.10683},
  note = {\url{https://arxiv.org/abs/1910.10683}},
}

@misc{gehring2017convolutionalsequencesequencelearning,
  title={Convolutional Sequence to Sequence Learning}, 
  author={Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
  year={2017},
  eprint={1705.03122},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1705.03122},
  note = {\url{https://arxiv.org/abs/1705.03122}},
}

@misc{transformervisuals,
  title={Transformer Visuals},
  author={Daniel Voigt Godoy (dvgodoy)},
  note = {\url{https://github.com/dvgodoy/dl-visuals/?tab=readme-ov-file}}
}

@misc{illustratedtransformer,
  title={The Illustrated Transformer},
  author={Jay Alammar},
  note = {\url{http://jalammar.github.io/illustrated-transformer/}}
}

@misc{dufter2021positioninformationtransformersoverview,
  title={Position Information in Transformers: An Overview}, 
  author={Philipp Dufter and Martin Schmitt and Hinrich Schütze},
  year={2021},
  eprint={2102.11090},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2102.11090},
  note = {\url{https://arxiv.org/abs/2102.11090}},
}

@misc{haviv2022transformerlanguagemodelspositional,
  title={Transformer Language Models without Positional Encodings Still Learn Positional Information}, 
  author={Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy},
  year={2022},
  eprint={2203.16634},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2203.16634},
  note = {\url{https://arxiv.org/abs/2203.16634}},
}

@misc{press2022trainshorttestlong,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}, 
  author={Ofir Press and Noah A. Smith and Mike Lewis},
  year={2022},
  eprint={2108.12409},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2108.12409},
  note = {\url{https://arxiv.org/abs/2108.12409}},
}

@misc{shaw2018selfattentionrelativepositionrepresentations,
  title={Self-Attention with Relative Position Representations}, 
  author={Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  year={2018},
  eprint={1803.02155},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1803.02155},
  note = {\url{https://arxiv.org/abs/1803.02155}},
}

@misc{ke2021rethinkingpositionalencodinglanguage,
  title={Rethinking Positional Encoding in Language Pre-training}, 
  author={Guolin Ke and Di He and Tie-Yan Liu},
  year={2021},
  eprint={2006.15595},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2006.15595},
  note = {\url{https://arxiv.org/abs/2006.15595}},
}

@misc{ding2023longnetscalingtransformers1000000000,
  title={LongNet: Scaling Transformers to 1,000,000,000 Tokens}, 
  author={Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Nanning Zheng and Furu Wei},
  year={2023},
  eprint={2307.02486},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2307.02486},
  note = {\url{https://arxiv.org/abs/2307.02486}},
}

@misc{mikolov2013distributedrepresentationswordsphrases,
  title={Distributed Representations of Words and Phrases and their Compositionality}, 
  author={Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
  year={2013},
  eprint={1310.4546},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1310.4546},
  note = {\url{https://arxiv.org/abs/1310.4546}},
}

@misc{kitaev2020reformerefficienttransformer,
  title={Reformer: The Efficient Transformer}, 
  author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
  year={2020},
  eprint={2001.04451},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2001.04451},
  note = {\url{https://arxiv.org/abs/2001.04451}},
}

@misc{dao2022flashattentionfastmemoryefficientexact,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
  author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
  year={2022},
  eprint={2205.14135},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2205.14135},
  note = {\url{https://arxiv.org/abs/2205.14135}},
}

@misc{chung2014empiricalevaluationgatedrecurrent,
  title={Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}, 
  author={Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
  year={2014},
  eprint={1412.3555},
  archivePrefix={arXiv},
  primaryClass={cs.NE},
  url={https://arxiv.org/abs/1412.3555},
  note = {\url{https://arxiv.org/abs/1412.3555}},
}

@misc{ragworkflow,
  title={RAG Workflow},
  author={LangChain},
  year={2024},
  note = {\url{https://github.com/langchain-ai/rag-from-scratch/tree/main}}
}

@misc{cohereembed,
  title={Cohere Embed},
  author={Cohere},
  year={2024},
  note = {\url{https://docs.cohere.com/docs/cohere-embed}}
}

@misc{coherererank,
  title={Cohere Rerank},
  author={Cohere},
  year={2024},
  note = {\url{https://docs.cohere.com/v2/docs/rerank-2}}
}

@misc{scikittsne,
  title={Scikit t-SNE},
  author={Scikit-Learn},
  note = {\url{https://scikit-learn.org/dev/modules/generated/sklearn.manifold.TSNE.html}}
}

@misc{roy2024trustworthydimensionalityreduction,
  title={Trustworthy Dimensionality Reduction}, 
  author={Subhrajyoty Roy},
  year={2024},
  eprint={2405.05868},
  archivePrefix={arXiv},
  primaryClass={stat.ME},
  url={https://arxiv.org/abs/2405.05868},
  note = {\url{https://arxiv.org/abs/2405.05868}},
}

@misc{zhang2024instructiontuninglargelanguage,
  title={Instruction Tuning for Large Language Models: A Survey}, 
  author={Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
  year={2024},
  eprint={2308.10792},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2308.10792},
  note = {\url{https://arxiv.org/abs/2308.10792}},
}

@misc{ouyang2022traininglanguagemodelsfollow,
  title={Training language models to follow instructions with human feedback}, 
  author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  year={2022},
  eprint={2203.02155},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2203.02155},
  note = {\url{https://arxiv.org/abs/2203.02155}},
}

@misc{whatisrag,
  title={What is RAG?},
  author={Amazon AWS},
  year={2023},
  note = {\url{https://aws.amazon.com/what-is/retrieval-augmented-generation/}}
}

@misc{raghandbook,
  title={Next-Gen Large Language Models: The Retrieval-Augmented Generation (RAG) Handbook},
  author={Vahe Aslanyan},
  year={2024},
  url={https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook},
  note={\url{https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook}}
}


@misc{jiang2023activeretrievalaugmentedgeneration,
  title={Active Retrieval Augmented Generation}, 
  author={Zhengbao Jiang and Frank F. Xu and Luyu Gao and Zhiqing Sun and Qian Liu and Jane Dwivedi-Yu and Yiming Yang and Jamie Callan and Graham Neubig},
  year={2023},
  eprint={2305.06983},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2305.06983},
  note = {\url{https://arxiv.org/abs/2305.06983}},
}

@article{Beurer_Kellner_2023,
  title={Prompting Is Programming: A Query Language for Large Language Models},
  volume={7},
  ISSN={2475-1421},
  url={http://dx.doi.org/10.1145/3591300},
  DOI={10.1145/3591300},
  number={PLDI},
  journal={Proceedings of the ACM on Programming Languages},
  publisher={Association for Computing Machinery (ACM)},
  author={Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
  year={2023},
  month=jun, pages={1946–1969},
  note = {\url{https://dl.acm.org/doi/10.1145/3591300}}
}

@misc{ragandfine,
  title={Fine tuning is for form, not facts},
  author={Anyscale},
  year={2023},
  note = {\url{https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts}}
}

@misc{lu2024finetuninglargelanguagemodels,
  title={Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities}, 
  author={Wei Lu and Rachel K. Luu and Markus J. Buehler},
  year={2024},
  eprint={2409.03444},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2409.03444},
  note = {\url{https://arxiv.org/abs/2409.03444}}
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
  title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
  author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
  year={2021},
  eprint={2005.11401},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2005.11401},
  note = {\url{https://arxiv.org/abs/2005.11401}}
}

@misc{zhao2024retrievalaugmentedgenerationrag,
  title={Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely}, 
  author={Siyun Zhao and Yuqing Yang and Zilong Wang and Zhiyuan He and Luna K. Qiu and Lili Qiu},
  year={2024},
  eprint={2409.14924},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2409.14924},
  note = {\url{https://arxiv.org/abs/2409.14924}}
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
  title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
  author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
  year={2024},
  eprint={2312.10997},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2312.10997},
  note = {\url{https://arxiv.org/abs/2312.10997}}
}

@misc{forer2024inferringscientificcrossdocumentcoreference,
  title={Inferring Scientific Cross-Document Coreference and Hierarchy with Definition-Augmented Relational Reasoning}, 
  author={Lior Forer and Tom Hope},
  year={2024},
  eprint={2409.15113},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2409.15113},
  note = {\url{https://arxiv.org/abs/2409.15113}}
}

@misc{liu2023lostmiddlelanguagemodels,
  title={Lost in the Middle: How Language Models Use Long Contexts}, 
  author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
  year={2023},
  eprint={2307.03172},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2307.03172},
  note = {\url{https://arxiv.org/abs/2307.03172}}
}

@misc{llmagents,
  title={LLM Agents},
  author={Prompt Engineering Guide},
  year={2023},
  note = {\url{https://www.promptingguide.ai/research/llm-agents}}
}

@misc{huggingfacehomepage,
  title={Hugging Face},
  author={Hugging Face},
  year={2024},
  note = {\url{https://huggingface.co}}
}

@misc{gemma22bit,
  title={Gemma2-2B-IT},
  author={Google},
  year={2024},
  note = {\url{https://huggingface.co/google/gemma-2-2b-it}}
}

@misc{llama38b,
  title={Llama3-8B},
  author={Meta Llama},
  year={2024},
  note = {\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B}}
}

@misc{quantization,
  title={Quantization},
  author={Hugging Face},
  year={2023},
  note = {\url{https://huggingface.co/docs/optimum/concept_guides/quantization}}
}

@misc{ollamasite,
  title={Ollama Website},
  author={Ollama},
  year={2024},
  note = {\url{https://ollama.com}}
}

@misc{ollamagithub,
  title={Ollama GitHub},
  author={Ollama},
  year={2024},
  note = {\url{https://github.com/ollama/ollama}}
}

@misc{quanttensorops,
  title={What are Quantized LLMs?},
  author={TensorOps},
  year={2023},
  note = {\url{https://www.tensorops.ai/post/what-are-quantized-llms}}
}

@misc{llama3ollama,
  title={Llama3-8B},
  author={Ollama},
  year={2024},
  note = {\url{https://ollama.com/library/llama3}}
}

@misc{langchain,
  title={LangChain},
  author={LangChain},
  year={2024},
  note = {\url{https://www.langchain.com/langchain}}
}

@misc{langchainragindexing,
  title={LangChain RAG Indexing},
  author={LangChain},
  year={2024},
  note = {\url{https://python.langchain.com/docs/tutorials/rag/}}
}

@misc{langgraph,
  title={LangGraph},
  author={LangChain},
  year={2024},
  note = {\url{https://www.langchain.com/langgraph}}
}

@misc{keanureeveswiki,
  title={Keanu Reeves},
  author={Wikipedia},
  year={2024},
  note = {\url{https://it.wikipedia.org/wiki/Keanu_Reeves}}
}

@misc{chromadb,
  title={ChromaDB GitHub},
  author={Chroma},
  year={2024},
  note = {\url{https://github.com/chroma-core/chroma}}
}

@misc{qdrant,
  title={Qdrant VectorStore},
  author={Qdrant},
  year={2024},
  note = {\url{https://qdrant.tech}}
}

@misc{faiss,
  title={Faiss},
  author={Facebook},
  year={2024},
  note = {\url{https://github.com/facebookresearch/faiss}}
}

@misc{douze2024faisslibrary,
  title={The Faiss library}, 
  author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},
  year={2024},
  eprint={2401.08281},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2401.08281},
  note = {\url{https://arxiv.org/abs/2401.08281}}
}

@misc{streamlit,
  title={Streamlit},
  author={Streamlit},
  year={2024},
  note = {\url{https://streamlit.io}}
}

@misc{llama318b,
  title={Llama3.1-8B},
  author={Meta Llama},
  year={2024},
  note = {\url{https://ollama.com/library/llama3.1}}
}

@misc{li2024documenttypeclassificationusing,
  title={Document Type Classification using File Names}, 
  author={Zhijian Li and Stefan Larson and Kevin Leach},
  year={2024},
  eprint={2410.01166},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2410.01166},
  note = {\url{https://arxiv.org/abs/2410.01166}}
}

@misc{mistralnemo12b,
  title={Mistral-Nemo 12B},
  author={Mistral AI and NVIDIA},
  year={2024},
  note = {\url{https://ollama.com/library/mistral-nemo}}
}

@misc{mukherjee2023orcaprogressivelearningcomplex,
  title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, 
  author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
  year={2023},
  eprint={2306.02707},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2306.02707},
  note = {\url{https://arxiv.org/abs/2306.02707}}
}

@misc{MACKIEWICZ1993303,
title = {Principal components analysis (PCA)},
journal = {Computers & Geosciences},
volume = {19},
number = {3},
pages = {303-342},
year = {1993},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(93)90090-R},
author = {Andrzej Maćkiewicz and Waldemar Ratajczak},
note = {\url{https://www.sciencedirect.com/science/article/abs/pii/009830049390090R}},
}

@misc{cai2022theoreticalfoundationstsnevisualizing,
  title={Theoretical Foundations of t-SNE for Visualizing High-Dimensional Clustered Data}, 
  author={T. Tony Cai and Rong Ma},
  year={2022},
  eprint={2105.07536},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/2105.07536},
  note = {\url{https://arxiv.org/abs/2105.07536}}
}

@misc{dolphin,
  title={Dolphin Mistral-Nemo 12B},
  author={Mistral AI and NVIDIA and cognitivecomputations},
  year={2024},
  note = {\url{https://huggingface.co/cognitivecomputations/dolphin-2.9.3-mistral-nemo-12b-gguf}}
}

@misc{fu2023complexitybasedpromptingmultistepreasoning,
  title={Complexity-Based Prompting for Multi-Step Reasoning}, 
  author={Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
  year={2023},
  eprint={2210.00720},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2210.00720},
  note = {\url{https://arxiv.org/abs/2210.00720}}
}

@misc{zheng2023secretsrlhflargelanguage,
  title={Secrets of RLHF in Large Language Models Part I: PPO}, 
  author={Rui Zheng and Shihan Dou and Songyang Gao and Yuan Hua and Wei Shen and Binghai Wang and Yan Liu and Senjie Jin and Qin Liu and Yuhao Zhou and Limao Xiong and Lu Chen and Zhiheng Xi and Nuo Xu and Wenbin Lai and Minghao Zhu and Cheng Chang and Zhangyue Yin and Rongxiang Weng and Wensen Cheng and Haoran Huang and Tianxiang Sun and Hang Yan and Tao Gui and Qi Zhang and Xipeng Qiu and Xuanjing Huang},
  year={2023},
  eprint={2307.04964},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2307.04964},
  note = {\url{https://arxiv.org/abs/2307.04964}}
}

@misc{ignitalia,
  title={IGN Italia},
  author={IGN},
  year={2024},
  note = {\url{https://it.ign.com}}
}

@misc{zelda,
  title={The Legend of Zelda: Tears of the Kingdom},
  author={Nintendo},
  year={2023},
  note = {\url{https://zelda.nintendo.com/tears-of-the-kingdom/}}
}

@misc{langsmith,
  title={LangSmith},
  author={LangChain},
  year={2024},
  note = {\url{https://smith.langchain.com}},
}

@misc{github,
  title={Profilo Github di Giuseppe Bellamacina},
  author={Giuseppe Bellamacina},
  year={2024},
  note = {\url{https://github.com/GiuseppeBellamacina}},
}

@article{Besta_2024,
  title={Graph of Thoughts: Solving Elaborate Problems with Large Language Models},
  volume={38},
  ISSN={2159-5399},
  url={http://dx.doi.org/10.1609/aaai.v38i16.29720},
  note = {\url{http://dx.doi.org/10.1609/aaai.v38i16.29720}},
  DOI={10.1609/aaai.v38i16.29720},
  number={16},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
  year={2024},
  month=mar, pages={17682–17690}
}

@misc{yao2023treethoughtsdeliberateproblem,
  title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
  author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  year={2023},
  eprint={2305.10601},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2305.10601},
  note = {\url{https://arxiv.org/abs/2305.10601}}
}

@misc{tavily,
  title={Tavily},
  author={Tavily},
  year={2024},
  note = {\url{https://tavily.com}}
}

@misc{yao2023reactsynergizingreasoningacting,
  title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  year={2023},
  eprint={2210.03629},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2210.03629},
  note = {\url{https://arxiv.org/abs/2210.03629}}
}

@misc{snell2024scalingllmtesttimecompute,
  title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
  author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
  year={2024},
  eprint={2408.03314},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2408.03314},
  note = {\url{https://arxiv.org/abs/2408.03314}},
}

@article{10.1093/pnasnexus/pgae233,
  author = {Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie C Y and Sheahan, Hannah R and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
  title = "{Language models, like humans, show content effects on reasoning tasks}",
  journal = {PNAS Nexus},
  volume = {3},
  number = {7},
  pages = {pgae233},
  year = {2024},
  month = {07},
  abstract = "{Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human reasoning is affected by our real-world knowledge and beliefs, and shows notable “content effects”; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns are central to debates about the fundamental nature of human intelligence. Here, we investigate whether language models—whose prior expectations capture some aspects of human knowledge—similarly mix content into their answers to logic problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art LMs, as well as humans, and find that the LMs reflect many of the same qualitative human patterns on these tasks—like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected in accuracy patterns, and in some lower-level features like the relationship between LM confidence over possible answers and human response times. However, in some cases the humans and models behave differently—particularly on the Wason task, where humans perform much worse than large models, and exhibit a distinct error pattern. Our findings have implications for understanding possible contributors to these human cognitive effects, as well as the factors that influence language model performance.}",
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgae233},
  url = {https://doi.org/10.1093/pnasnexus/pgae233},
  note = {\url{https://doi.org/10.1093/pnasnexus/pgae233}},
  eprint = {https://academic.oup.com/pnasnexus/article-pdf/3/7/pgae233/58651606/pgae233.pdf},
}

@misc{shevlane2023modelevaluationextremerisks,
  title={Model evaluation for extreme risks}, 
  author={Toby Shevlane and Sebastian Farquhar and Ben Garfinkel and Mary Phuong and Jess Whittlestone and Jade Leung and Daniel Kokotajlo and Nahema Marchal and Markus Anderljung and Noam Kolt and Lewis Ho and Divya Siddarth and Shahar Avin and Will Hawkins and Been Kim and Iason Gabriel and Vijay Bolina and Jack Clark and Yoshua Bengio and Paul Christiano and Allan Dafoe},
  year={2023},
  eprint={2305.15324},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2305.15324},
  note = {\url{https://arxiv.org/abs/2305.15324}},
}

@article{Wang_2024,
  title={A survey on large language model based autonomous agents},
  volume={18},
  ISSN={2095-2236},
  url={http://dx.doi.org/10.1007/s11704-024-40231-1},
  note = {\url{http://dx.doi.org/10.1007/s11704-024-40231-1}},
  DOI={10.1007/s11704-024-40231-1},
  number={6},
  journal={Frontiers of Computer Science},
  publisher={Springer Science and Business Media LLC},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
  year={2024},
  month=mar
}

@misc{intellisync,
  title={Intellisync},
  author={Intellisync},
  year={2024},
  note = {\url{https://www.intellisync.it}}
}

@misc{arcane,
  title={Arcane},
  author={Riot Games and Netflix},
  year={2024},
  note = {\url{https://www.arcane.com/it-it/}}
}

@misc{cowboy,
  title={Cowboy Bebop},
  author={Sunrise and Bandai Namco},
  year={2024},
  note = {\url{https://cowboy-bebop.net}}
}

@misc{xi2023risepotentiallargelanguage,
  title={The Rise and Potential of Large Language Model Based Agents: A Survey}, 
  author={Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
  year={2023},
  eprint={2309.07864},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2309.07864},
  note = {\url{https://arxiv.org/abs/2309.07864}},
}

@misc{guo2024largelanguagemodelbased,
  title={Large Language Model based Multi-Agents: A Survey of Progress and Challenges}, 
  author={Taicheng Guo and Xiuying Chen and Yaqi Wang and Ruidi Chang and Shichao Pei and Nitesh V. Chawla and Olaf Wiest and Xiangliang Zhang},
  year={2024},
  eprint={2402.01680},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2402.01680},
  note = {\url{https://arxiv.org/abs/2402.01680}},
}

@misc{shao2024assistingwritingwikipedialikearticles,
  title={Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}, 
  author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},
  year={2024},
  eprint={2402.14207},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2402.14207},
  note = {\url{https://arxiv.org/abs/2402.14207}},
}